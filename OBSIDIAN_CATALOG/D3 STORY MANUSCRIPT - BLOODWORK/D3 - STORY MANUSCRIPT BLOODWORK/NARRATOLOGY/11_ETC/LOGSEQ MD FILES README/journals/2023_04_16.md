  public:: true
  
- # LEECHSEED 063
- ![image.png](../assets/image_1681899845715_0.png)
- [LEECHSEED 063 SOUNDCLOUD LINK](https://soundcloud.com/leechseedaudio3/leechseed-063/s-BGVBsr3kvmp?si=2a65c5775c8449a090725dcd7dd9542a&utm_source=clipboard&utm_medium=text&utm_campaign=social_sharing)
- *original recording date 4.16.23, upload date 4.19.23*
- Alright let's meet sheet 63 we're gonna try something a little bit different this time did the research when Hogan faced on that shit and I'm just trying to really just drill down what exactly I need for this program to do right what I need from it I'm going through the poll process of what I've done I think the easiest way to explain this right I need an AI urban yes yes I'd be a model that takes inputs from that saying screenshots I take a text right Joe a page of text PDF page I guess of course this can be like I don't know but anyway so I would just take a screenshot and control shift as and then to draw the shape or in the text that I want and I'll create a series of these files right a whole pop folder these files are actually know I'm gon to do and put one at a time right I'll take the screen short that I want I'd like to paste it ideally and to whatever ABI to then of the masking I'm asking API of a masculine model the appeal high of a masking market and then take that and put them generate the text of it as you output text then what we put into a plane text file I guess I'm inot gonna be ideal or without put just you know the text and I would copy it actually yeah I'll put the text that I would copy it in 2 battery that's right 7% a lock sack I mean that would say 2 seconds ago she yet I guess it isn't about the time savings as much as it is the sort of physical psychotic effort it takes to actually do the correct highlighting copy pasting but then again there can also be just to get better at it as much as I do it you know I guess it all puts in the need for automation right like why The more I look into it the more they are read about this hugging face things like I understand now how these models work and such and it's super specific with all this shit I haven't even been a python and I realized honestly there might be a little bit of drift and what I'm trying to you and by drift I mean guilt Perhaps I feel guilty for doing this at the end of the day for wanting to use AI and my systemic processing of writing or writing whatever I think about it also learned I can still adjust the process of decoding this book then told him decode I also did learn a lot of this awarding a lot of the vocabulary a lot of the key words used to sort to describe what exactly I'm wanting to do The thing is I still kind of want the full real text put iblock Zach cause I know I would never really need the PDFs I have them but it's all Britain locks back with the search algorithm and everything it's fine I guess at the end of the day a lot of this feels like emotional productivity which I went down this rabbit hole because I feel like I have to be justified to do it it has to be a righteous effort to do this right Babe to be able to explain this process when they confront you about it And the sort of misguided assumption assumption of cheating I'm easier really what this is and the way it's better and in a way I also know the limitations of this but then I think a lot of these invitations doesn't necessarily apply to what I'm try to do and there's still a little bit of hand work done into this right so I'm doing a kitchen you can do everything by scratch everything you can be like the French laundry where you have Jesus Christ of garden in your back fucking porch and use all your vegetables from your garden the limitation is a season of the year you have to keep changing your menu it's all gonna be great of course but don't talk about one thing and it all depends on what you want to do it's your restaurant it's my book it's my life it's my prerogative what you say I don't care what the matters is yet an operation separate the book from the authors do it if you want Instead of 21 I was doing before I'm doing multiple passes myself i learned about that what is it called i remember these items like a key where identification but it's it what this model operation what challenged between you do is connect it can identify keywords it's as in the vocabuland you blocks and uh could you hurt itself I could automate that again I think doing it by hand again was another operation of guilt that I felt like I had to do it to prove a point and I think I'm right I don't want to make myself feel guilty for acting on guilt and so fucking vicious circle of bullshit with the idea I bullets it's righteous in my opinion that I went through all this because I couldn't just kept my head down kept trucking along and just not be smart about it which I want to be smart about it probably At the end of the day what it is is extracting information from the book and understanding it so I can make what I want to make or wanna make is great frosted flakes motherfucker out
- # Hugging Face Course
- ## For my purpose of taking a book and summarizing it, the specific type of transformer model I would use is:
- ^^Summarizing^^
- ### for the secondary objective of extracting key words I would use the transformer model:
- ^^Named Entity Recognition (NER)^^
- ## I am interested in **BERT-like transformers, Captain**
- ### As well as *encoder-decoder models* or *sequence-to-sequence models*
- #### However, now knowing what an Encoder model does individually, an Encoder model might be best for identifying Key Words. Though, it seems that sequence-to-sequence models are so advanced nowadays it probably doesn't even matter.
- In terms of Datasets, it seems that models trained on 'neutral data' are the ones I'm interested in such as Wikipedia and BookCorpus
- ![image.png](../assets/image_1681641103744_0.png)
- # Now that I know the model transformer I want to use, its now time to choose an appropriate transformer
- ## The TreeTagger and Stanford Tokenizers specialize in academic and scientific texts. However, it might be a good idea to consider using a general use tokenizer like SpaCy or idk whatever's easier.
- TreeTagger tokenizer
- Stanford Tokenizer
- # maybe what I really need to train is a masking model
- ## the model will be trained with pdfs that I collect and take that text  output them into plain text
- ### then personally, myself, I, will read portions of the book and determine the appropriate word count (character count) input should be for (a python script to break apart the plain text file into data chunks appropriate for, lets cut the shit, chat gpt)
-
-
- # Code Academy Python 3 course
	- [cheat shit](https://www.codecademy.com/learn/learn-python-3/modules/learn-python3-hello-world/cheatsheet)
	-
